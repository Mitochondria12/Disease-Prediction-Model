{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "Dataset = pd.read_csv(\"Training.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "Dataset = Dataset.dropna()  # Remove any rows with null values\n",
    "Dataset = Dataset.drop(\"Id\", axis=1)  # Remove the \"Id\" column as it's just sample information\n",
    "\n",
    "# Separate the target label (\"Class\") from the dataset\n",
    "Labels = Dataset[[\"Class\"]].copy()  # Create a separate dataframe for class labels\n",
    "Dataset = Dataset.drop(\"Class\", axis=1)  # Remove the \"Class\" column from the main dataset\n",
    "\n",
    "# Convert string labels in 'EJ' column to integers\n",
    "mapping = {'A': 1, 'B': 0}\n",
    "Dataset['EJ'] = Dataset['EJ'].replace(mapping)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "Dataset_train, Dataset_test, Labels_train, Labels_test = train_test_split(Dataset, Labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Use SMOTE (Synthetic Minority Over-sampling Technique) to handle class imbalance\n",
    "sm = SMOTE(random_state=42)\n",
    "AltDataset, AltLabels = sm.fit_resample(Dataset_train, Labels_train)\n",
    "\n",
    "# Convert Pandas dataframes to TensorFlow tensors\n",
    "AltDataset = tf.convert_to_tensor(AltDataset.values, dtype=tf.float32)\n",
    "AltLabels = tf.convert_to_tensor(AltLabels.values, dtype=tf.float32)\n",
    "\n",
    "# Define the neural network model\n",
    "DiseaseModel = keras.Sequential([\n",
    "    layers.Dense(300, activation=\"relu\"),  # Hidden layer with 300 neurons and ReLU activation\n",
    "    layers.Dense(1, activation=\"sigmoid\")  # Output layer with 1 neuron (binary classification) and sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model with appropriate optimizer, loss function, and evaluation metric\n",
    "DiseaseModel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model using the oversampled training data\n",
    "DiseaseModel.fit(AltDataset, AltLabels, epochs=30, batch_size=150)\n",
    "\n",
    "# Evaluate the model's performance on test data\n",
    "print(\"I will determine the accuracy of my test data.\")\n",
    "Evaluations = DiseaseModel.evaluate(Dataset_test, Labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Function to create a simple neural network model\n",
    "def create_model():\n",
    "    model = Sequential([Dense(140, activation=\"relu\"),Dense(30, activation=\"swish\"), Dense(1, activation=\"sigmoid\")])\n",
    "    return model\n",
    "\n",
    "# Define a binary crossentropy loss function\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Number of models to initialize\n",
    "num_models = 100\n",
    "\n",
    "# Create the list of models\n",
    "models = [create_model() for _ in range(num_models)]\n",
    "\n",
    "# Loop through generations and train models\n",
    "for generation in range(min(50, len(AltDataset))):\n",
    "    losses = []\n",
    "\n",
    "    # Train each model on a sample from the dataset\n",
    "    for model in models:\n",
    "        sample, label = AltDataset[generation], AltLabels[generation]\n",
    "        optimizer = tf.keras.optimizers.Adam()  # Initialize the optimizer\n",
    "        \n",
    "        # Compute the gradients and apply them\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = model(sample[None, ...], training=True)\n",
    "            loss = loss_function(label[None, ...], prediction)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        # Record the loss for each model\n",
    "        losses.append(loss)\n",
    "\n",
    "    # Sort the models based on their losses\n",
    "    sorted_indices = np.argsort(losses)\n",
    "    top_20_idx = sorted_indices[:len(models) * 20 // 100]\n",
    "    mid_40_idx = sorted_indices[len(models) * 20 // 100:len(models) * 60 // 100]\n",
    "    bottom_40_idx = sorted_indices[len(models) * 60 // 100:]\n",
    "\n",
    "    # Crossover function to create new models from pairs of parents\n",
    "    def crossover_and_create_models(parents, num_children_per_pair):\n",
    "        new_models = []\n",
    "        num_pairs = len(parents) // 2 * 2  # Ensure even number of parents\n",
    "        for i in range(0, num_pairs, 2):\n",
    "            for _ in range(num_children_per_pair):\n",
    "                new_model = create_model()\n",
    "                new_model.build((None, sample.shape[0]))\n",
    "                # Combine weights from two parent models to create a new model\n",
    "                for j in range(len(new_model.weights)):\n",
    "                    mask = np.random.choice([0, 1], size=new_model.weights[j].shape.as_list())\n",
    "                    new_weights = np.where(mask, models[parents[i]].weights[j].numpy(), models[parents[i + 1]].weights[j].numpy())\n",
    "                    new_model.weights[j].assign(new_weights)\n",
    "                new_models.append(new_model)\n",
    "        return new_models\n",
    "\n",
    "    # Use crossover function to create new models\n",
    "    new_models = crossover_and_create_models(sorted_indices, 4)\n",
    "\n",
    "    # Set the models list to the newly created models for the next generation\n",
    "    models = new_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models)\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "for i, model in enumerate(models):  \n",
    "      y_pred = model.predict(Dataset_test)  # Assuming Dataset_test is your test data\n",
    "      y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to class labels (0 or 1)\n",
    "\n",
    "      conf_matrix = np.zeros((2, 2), dtype=int)\n",
    "      conf_matrix[0, 0] = np.sum((Labels_test.values == 0) & (y_pred == 0))  # True Negatives (TN)\n",
    "      conf_matrix[0, 1] = np.sum((Labels_test.values == 0) & (y_pred == 1))  # False Positives (FP)\n",
    "      conf_matrix[1, 0] = np.sum((Labels_test.values == 1) & (y_pred == 0))  # False Negatives (FN)\n",
    "      conf_matrix[1, 1] = np.sum((Labels_test.values == 1) & (y_pred == 1))  # True Positives (TP)\n",
    "\n",
    "      # Extract true negatives (TN), false positives (FP), true positives (TP), and false negatives (FN) from the confusion matrix\n",
    "      TN, FP, TP, FN = conf_matrix[0, 0], conf_matrix[0, 1], conf_matrix[1, 1], conf_matrix[1, 0]\n",
    "\n",
    "      # Calculate specificity, sensitivity, precision, NPV, and accuracy\n",
    "      specificity = TN / (TN + FP)\n",
    "      sensitivity = TP / (TP + FN)\n",
    "      precision_class_one = TP / (TP + FP)\n",
    "      Negative_Predictive_Value = TN / (TN + FN)\n",
    "      accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "      # Print the metrics\n",
    "      \"\"\"\n",
    "      print(f\"Model {i+1}\")\n",
    "      print(f\"Sensitivity: {sensitivity * 100:.2f}%,\",\n",
    "            f\"Specificity: {specificity * 100:.2f}%,\",\n",
    "            f\"Accuracy: {accuracy * 100:.2f}%,\",\n",
    "            f\"Negative Predictive Value: {Negative_Predictive_Value * 100:.2f}%,\",\n",
    "            f\"Precision (Class One): {precision_class_one * 100:.2f}%\")\n",
    "      print(f\"True Negatives (TN): {TN},\",\n",
    "            f\"False Positives (FP): {FP},\",\n",
    "            f\"True Positives (TP): {TP},\",\n",
    "            f\"False Negatives (FN): {FN}\")\n",
    "      \"\"\"\n",
    "\n",
    "      if sensitivity > 0.3 and specificity >0.8:\n",
    "            print(\"Green\")\n",
    "            print(f\"Model {i+1}\")\n",
    "            print(f\"Sensitivity: {sensitivity * 100:.2f}%,\",\n",
    "                  f\"Specificity: {specificity * 100:.2f}%,\",\n",
    "                  f\"Accuracy: {accuracy * 100:.2f}%,\",\n",
    "                  f\"Negative Predictive Value: {Negative_Predictive_Value * 100:.2f}%,\",\n",
    "                  f\"Precision (Class One): {precision_class_one * 100:.2f}%\")\n",
    "            print(f\"True Negatives (TN): {TN},\",\n",
    "                  f\"False Positives (FP): {FP},\",\n",
    "                  f\"True Positives (TP): {TP},\",\n",
    "                  f\"False Negatives (FN): {FN}\")\n",
    "\n",
    "Proportion = Labels_test[\"Class\"].value_counts()\n",
    "print(\"By random chance the precision would be {:.2f}%\".format(Proportion[1] / (Proportion[0] + Proportion[1]) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming y_true and y_pred are numpy arrays representing the true labels and predicted labels, respectively.\n",
    "# 1 represents the positive class, and 0 represents the negative class.\n",
    "y_pred=model.predict(Dataset_test)\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = np.zeros((2, 2), dtype=int)\n",
    "conf_matrix[0, 0] = np.sum((Labels_test.values == 0) & (y_pred == 0))  # True Negatives (TN)\n",
    "conf_matrix[0, 1] = np.sum((Labels_test.values== 0) & (y_pred == 1))  # False Positives (FP)\n",
    "conf_matrix[1, 0] = np.sum((Labels_test.values == 1) & (y_pred == 0))  # False Negatives (FN)\n",
    "conf_matrix[1, 1] = np.sum((Labels_test.values == 1) & (y_pred == 1))  # True Positives (TP)\n",
    "\n",
    "# Extract true negatives (TN) and false positives (FP) from the confusion matrix\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "TP = conf_matrix[1, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "# Calculate specificity\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "precision_class_one=TP/(TP+FP)\n",
    "Negative_Predictive_Value=TN/(TN+FN)\n",
    "accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
    "print(f\"Specificity: {specificity * 100:.2f}%\",\n",
    "      f\"Sensitivity: {sensitivity * 100:.2f}%\",\n",
    "      f\"Accuracy: {accuracy * 100:.2f}%\",\n",
    "      f\"Negative Predictive Value: {Negative_Predictive_Value * 100:.2f}%\",\n",
    "      f\"Precision (Class One): {precision_class_one * 100:.2f}%\")\n",
    "print(f\"True Negatives (TN): {TN}\",\n",
    "      f\"False Positives (FP): {FP}\",\n",
    "      f\"True Positives (TP): {TP}\",\n",
    "      f\"False Negatives (FN): {FN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming `model` is your trained model\n",
    "# Assuming `X_test` is your test data and `y_test` are your test labels\n",
    "y_pred = DiseaseModel.predict(Dataset_test)  # assuming x_test is your test data\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "conf_matrix = np.zeros((2, 2), dtype=int)\n",
    "conf_matrix[0, 0] = np.sum((Labels_test.values == 0) & (y_pred == 0))  # True Negatives (TN)\n",
    "conf_matrix[0, 1] = np.sum((Labels_test.values== 0) & (y_pred == 1))  # False Positives (FP)\n",
    "conf_matrix[1, 0] = np.sum((Labels_test.values == 1) & (y_pred == 0))  # False Negatives (FN)\n",
    "conf_matrix[1, 1] = np.sum((Labels_test.values == 1) & (y_pred == 1))  # True Positives (TP)\n",
    "\n",
    "# Extract true negatives (TN) and false positives (FP) from the confusion matrix\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "TP = conf_matrix[1, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "# Calculate specificity\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "precision_class_one=TP/(TP+FP)\n",
    "Negative_Predictive_Value=TN/(TN+FN)\n",
    "accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
    "print(specificity*100,sensitivity*100,accuracy*100,Negative_Predictive_Value*100,precision_class_one*100)\n",
    "print(TN,FP,TP,FN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
