{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "Dataset= pd.read_csv(\"Training.csv\")\n",
    "\n",
    "\n",
    "Dataset=Dataset.dropna() # remove null values\n",
    "Dataset=Dataset.drop(\"Id\",axis=1) # remove sample information\n",
    "#print(Dataset.value_counts(Dataset[\"Class\"]))\n",
    "Labels=Dataset[[\"Class\"]].copy() # generate separate class dataframe\n",
    "Dataset=Dataset.drop(\"Class\",axis=1) # drop class from feature dataframe\n",
    "mapping = {'A': 1, 'B': 0} # change object from str to int \n",
    "Dataset['EJ'] = Dataset['EJ'].replace(mapping)\n",
    "#print(Dataset.shape)\n",
    "Dataset_train, Dataset_test, Labels_train, Labels_test = train_test_split(Dataset, Labels, test_size=0.5, random_state=42)\n",
    "#print(type(Dataset_train))\n",
    "sm = SMOTE(random_state=42)\n",
    "AltDataset, AltLabels = sm.fit_resample(Dataset_train, Labels_train)\n",
    "#print(AltLabels.value_counts(AltLabels[\"Class\"]))\n",
    "AltDataset = tf.convert_to_tensor(AltDataset.values, dtype=tf.float32)\n",
    "AltLabels=tf.convert_to_tensor(AltLabels.values, dtype=tf.float32)\n",
    "print(len(Labels_train))\n",
    "DiseaseModel=keras.Sequential([layers.Dense(300,activation=\"relu\"),layers.Dense(1,activation=\"sigmoid\")])\n",
    "DiseaseModel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "DiseaseModel.fit(AltDataset,AltLabels,epochs=30,batch_size=150)\n",
    "print(\"I will determine the accuracy of my test data.\")\n",
    "Evaluations=DiseaseModel.evaluate(Dataset_test,Labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([Dense(140, activation=\"relu\"),Dense(30, activation=\"swish\"), Dense(1, activation=\"sigmoid\")])\n",
    "    return model\n",
    "\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "population_size=100\n",
    "\n",
    "\n",
    "population=[[create_model() for amount in range(2)] for size in range(population_size)]\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam()\n",
    "\n",
    "\n",
    "for generation in range(min(10, len(AltDataset))):  # For each generation\n",
    "    losses = []\n",
    "    for Individual in population:\n",
    "        Individual_Loss = 0\n",
    "        sample, label = AltDataset[generation], AltLabels[generation]\n",
    "        for model in Individual:\n",
    "            with tf.GradientTape() as tape:\n",
    "                prediction = model(sample[None, ...], training=True)\n",
    "                loss = loss_function(label[None, ...], prediction)\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # Use the same optimizer instance\n",
    "\n",
    "            Individual_Loss += loss    \n",
    "        losses.append(Individual_Loss/2)\n",
    "\n",
    "\n",
    "    top_individuals_indices= (np.argsort(losses))[0:int((len(population)/2)+1)]\n",
    "    #The following line identifies the positions of the individuals (models) \n",
    "    # with the lowest losses in the population. Specifically, it selects just over \n",
    "    # half of the population, ranking them by their loss values. This subset of \n",
    "    # indices can be used to select the top-performing individuals for further \n",
    "    # genetic operations such as crossover and mutation, thereby ensuring that \n",
    "    # the traits of the best-performing models are carried over to the next generation.\n",
    "    \n",
    "    \n",
    "    def generate_haploid_cluster(individual, cluster_size): \n",
    "        # This function takes an individual organism containing two models (analogous to paired chromosomes) \n",
    "        # and generates a cluster of gametes (like sperm), each gamete is a single model which is a combination of the two models (analogous to process of meiosis).\n",
    "\n",
    "        haploid_cluster = [] # List to store the generated gametes\n",
    "\n",
    "        for gamete_index in range(cluster_size): # Loop to create the desired number of gametes\n",
    "            paired_chromosomes = individual # Retrieve the two models representing the individual's paired chromosomes\n",
    "            gamete = create_model() # Create a new model with the same architecture\n",
    "            gamete.build((None, sample.shape[0])) # Initialize the weights of the gamete model based on the anticipated input size\n",
    "\n",
    "            # Loop through the weight tensors in the gamete model\n",
    "            for weight_index in range(len(gamete.weights)): \n",
    "                # Create a mask of random 0s and 1s to determine which genetic material to inherit from each paired chromosome\n",
    "                mask = np.random.choice([0, 1], size=gamete.weights[weight_index].shape.as_list())\n",
    "                \n",
    "                # Use the mask to combine genetic material from both paired chromosomes, element by element\n",
    "                combined_weights = np.where(mask, paired_chromosomes[0].weights[weight_index].numpy(), paired_chromosomes[1].weights[weight_index].numpy())\n",
    "\n",
    "                # Assign the combined genetic material to the gamete model\n",
    "                gamete.weights[weight_index].assign(combined_weights)\n",
    "\n",
    "            haploid_cluster.append(gamete) # Add the gamete to the cluster\n",
    "\n",
    "        return haploid_cluster # Return the cluster of gametes\n",
    "    \n",
    "\n",
    "    def gamete_selection(haploid_cluster, generation, num_gametes=4):\n",
    "        sample, label = AltDataset[0:generation], AltLabels[0:generation]\n",
    "\n",
    "        gametes_fitness_scores = []\n",
    "        for gamete in haploid_cluster:\n",
    "            with tf.GradientTape() as tape:\n",
    "                prediction = gamete(sample, training=True)\n",
    "                loss_fitness_score = loss_function(label, prediction)\n",
    "            gametes_fitness_scores.append((gamete, loss_fitness_score))\n",
    "            \n",
    "        # Sort the gametes by their fitness scores and select the top N\n",
    "        sorted_gametes = sorted(gametes_fitness_scores, key=lambda x: x[1])[:num_gametes]\n",
    "        return [gamete[0] for gamete in sorted_gametes]\n",
    "\n",
    "\n",
    "    def offspring_fertilisation(parent_one_gamete,parent_two_gamete): #Two gametes come together to form a whole individual with a complete set of chromosomes (two models)\n",
    "        offspring=[parent_one_gamete,parent_two_gamete]\n",
    "        return(offspring)\n",
    "\n",
    "       # Create the next generation\n",
    "    count = 0\n",
    "\n",
    "    next_generation = []  # Make sure to initialize the next_generation list\n",
    "    for i in range(len(top_individuals_indices) - 1):  # Pairing individuals\n",
    "        # Only proceed with even counts\n",
    "        if count % 2 != 0:\n",
    "            count += 1\n",
    "            continue\n",
    "\n",
    "        # Generate haploid clusters for parent individuals\n",
    "        haploid_cluster_parent_one = generate_haploid_cluster(population[top_individuals_indices[count]], cluster_size=10)\n",
    "        haploid_cluster_parent_two = generate_haploid_cluster(population[top_individuals_indices[count + 1]], cluster_size=10)\n",
    "\n",
    "        # Select the best gametes from each cluster\n",
    "        parent_one_gamete = gamete_selection(haploid_cluster_parent_one, generation)\n",
    "        parent_two_gamete = gamete_selection(haploid_cluster_parent_two, generation)\n",
    "\n",
    "        # Fertilize the selected gametes to create offspring\n",
    "        offspring1 = offspring_fertilisation(parent_one_gamete[0], parent_two_gamete[0])\n",
    "        offspring2 = offspring_fertilisation(parent_one_gamete[1], parent_two_gamete[1])\n",
    "        offspring3 = offspring_fertilisation(parent_one_gamete[2],parent_two_gamete[2])\n",
    "        offspring4 = offspring_fertilisation(parent_one_gamete[3],parent_two_gamete[3])\n",
    "        # Add the offspring to the next generation\n",
    "        next_generation.append(offspring1)\n",
    "        next_generation.append(offspring2)\n",
    "        next_generation.append(offspring3)\n",
    "        next_generation.append(offspring4)\n",
    "        count += 1\n",
    "    population=next_generation\n",
    "    print(population)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(population)\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "for i, individual in enumerate(population):  \n",
    "      specificity=0\n",
    "      sensitivity=0\n",
    "      precision_class_one=0\n",
    "      Negative_Predictive_Value=0\n",
    "      accuracy=0\n",
    "      for model in individual:\n",
    "            y_pred = model.predict(Dataset_test)  # Assuming Dataset_test is your test data\n",
    "            y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to class labels (0 or 1)\n",
    "\n",
    "            conf_matrix = np.zeros((2, 2), dtype=int)\n",
    "            conf_matrix[0, 0] = np.sum((Labels_test.values == 0) & (y_pred == 0))  # True Negatives (TN)\n",
    "            conf_matrix[0, 1] = np.sum((Labels_test.values == 0) & (y_pred == 1))  # False Positives (FP)\n",
    "            conf_matrix[1, 0] = np.sum((Labels_test.values == 1) & (y_pred == 0))  # False Negatives (FN)\n",
    "            conf_matrix[1, 1] = np.sum((Labels_test.values == 1) & (y_pred == 1))  # True Positives (TP)\n",
    "\n",
    "            # Extract true negatives (TN), false positives (FP), true positives (TP), and false negatives (FN) from the confusion matrix\n",
    "            TN, FP, TP, FN = conf_matrix[0, 0], conf_matrix[0, 1], conf_matrix[1, 1], conf_matrix[1, 0]\n",
    "\n",
    "            # Calculate specificity, sensitivity, precision, NPV, and accuracy\n",
    "            specificity = specificity+(TN / (TN + FP))\n",
    "            sensitivity = sensitivity+(TP / (TP + FN))\n",
    "            precision_class_one = precision_class_one+(TP / (TP + FP))\n",
    "            Negative_Predictive_Value =Negative_Predictive_Value+( TN / (TN + FN))\n",
    "            accuracy =accuracy+((TP + TN) / (TP + TN + FP + FN))\n",
    "\n",
    "            \n",
    "      # Print the metrics\n",
    "      #\"\"\"\n",
    "      print(f\"Individual {i+1}\")\n",
    "      print(f\"Sensitivity: {sensitivity/2 * 100:.2f}%,\",\n",
    "            f\"Specificity: {specificity/2 * 100:.2f}%,\",\n",
    "            f\"Accuracy: {accuracy/2 * 100:.2f}%,\",\n",
    "            f\"Negative Predictive Value: {Negative_Predictive_Value/2 * 100:.2f}%,\",\n",
    "            f\"Precision (Class One): {precision_class_one/2 * 100:.2f}%\")\n",
    "      print(f\"True Negatives (TN): {TN},\",\n",
    "            f\"False Positives (FP): {FP},\",\n",
    "            f\"True Positives (TP): {TP},\",\n",
    "            f\"False Negatives (FN): {FN}\")\n",
    "      #\"\"\"\n",
    "\n",
    "\n",
    "Proportion = Labels_test[\"Class\"].value_counts()\n",
    "print(\"By random chance the precision would be {:.2f}%\".format(Proportion[1] / (Proportion[0] + Proportion[1]) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming y_true and y_pred are numpy arrays representing the true labels and predicted labels, respectively.\n",
    "# 1 represents the positive class, and 0 represents the negative class.\n",
    "y_pred=model.predict(Dataset_test)\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = np.zeros((2, 2), dtype=int)\n",
    "conf_matrix[0, 0] = np.sum((Labels_test.values == 0) & (y_pred == 0))  # True Negatives (TN)\n",
    "conf_matrix[0, 1] = np.sum((Labels_test.values== 0) & (y_pred == 1))  # False Positives (FP)\n",
    "conf_matrix[1, 0] = np.sum((Labels_test.values == 1) & (y_pred == 0))  # False Negatives (FN)\n",
    "conf_matrix[1, 1] = np.sum((Labels_test.values == 1) & (y_pred == 1))  # True Positives (TP)\n",
    "\n",
    "# Extract true negatives (TN) and false positives (FP) from the confusion matrix\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "TP = conf_matrix[1, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "# Calculate specificity\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "precision_class_one=TP/(TP+FP)\n",
    "Negative_Predictive_Value=TN/(TN+FN)\n",
    "accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
    "print(f\"Specificity: {specificity * 100:.2f}%\",\n",
    "      f\"Sensitivity: {sensitivity * 100:.2f}%\",\n",
    "      f\"Accuracy: {accuracy * 100:.2f}%\",\n",
    "      f\"Negative Predictive Value: {Negative_Predictive_Value * 100:.2f}%\",\n",
    "      f\"Precision (Class One): {precision_class_one * 100:.2f}%\")\n",
    "print(f\"True Negatives (TN): {TN}\",\n",
    "      f\"False Positives (FP): {FP}\",\n",
    "      f\"True Positives (TP): {TP}\",\n",
    "      f\"False Negatives (FN): {FN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming `model` is your trained model\n",
    "# Assuming `X_test` is your test data and `y_test` are your test labels\n",
    "y_pred = DiseaseModel.predict(Dataset_test)  # assuming x_test is your test data\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "conf_matrix = np.zeros((2, 2), dtype=int)\n",
    "conf_matrix[0, 0] = np.sum((Labels_test.values == 0) & (y_pred == 0))  # True Negatives (TN)\n",
    "conf_matrix[0, 1] = np.sum((Labels_test.values== 0) & (y_pred == 1))  # False Positives (FP)\n",
    "conf_matrix[1, 0] = np.sum((Labels_test.values == 1) & (y_pred == 0))  # False Negatives (FN)\n",
    "conf_matrix[1, 1] = np.sum((Labels_test.values == 1) & (y_pred == 1))  # True Positives (TP)\n",
    "\n",
    "# Extract true negatives (TN) and false positives (FP) from the confusion matrix\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "TP = conf_matrix[1, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "# Calculate specificity\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "precision_class_one=TP/(TP+FP)\n",
    "Negative_Predictive_Value=TN/(TN+FN)\n",
    "accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
    "print(sensitivity*100,specificity*100,accuracy*100,Negative_Predictive_Value*100,precision_class_one*100)\n",
    "print(TN,FP,TP,FN)\n",
    "#Sensitivity: 44.90%, Specificity: 82.22%, Accuracy: 75.55%, Negative Predictive Value: 87.26%, Precision (Class One): 35.48%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
